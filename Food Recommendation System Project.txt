from google.colab import drive
drive.mount('/content/drive')
import os
os.environ["kaggle_com"]="/content/drive/MyDrive/Kaggle"
%cd /content/drive/MyDrive/Kaggle
import pandas as pd
df= pd.read_csv('/content/drive/MyDrive/Kaggle/Food (1).csv')
df.head()
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.decomposition import NMF
from sklearn.feature_extraction.text import TfidfVectorizer
print(df.isnull().sum())  # Print the count of missing values in each column
print(df.duplicated().sum())  # Print the count of duplicate rows
import matplotlib.pyplot as plt
labels = ['A', 'B', 'C', 'D', 'E']
values = [10, 15, 7, 12, 9]
plt.bar(labels, values)
plt.xlabel('Categories')
plt.ylabel('Values')
plt.title('Bar Chart')
plt.show()
Ratings = df['Rating'].tolist()
import pandas as pd
import matplotlib.pyplot as plt
# Visualize the rating distribution
plt.hist(Ratings, bins=5, edgecolor='black')
plt.xlabel('Rating')
plt.ylabel('Frequency')
plt.title('Rating Distribution')
plt.show()
Q1 = df['Calories'].quantile(0.25)
Q3 = df['Calories'].quantile(0.75)
IQR = Q3 - Q1
df = df[(df['Calories'] > Q1 - 1.5 * IQR) & (df['Calories'] < Q3 + 1.5 * IQR)]
!pip install scikit-learn
!pip install pandas
from sklearn.decomposition import PCA
tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix = tfidf_vectorizer.fit_transform(df['Name'] + ' ' + df['C_Type'] + ' ' + df['Ingredients'] + ' ' + df['Diet Restriction'] + ' ' + df['Seasonality'])
cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)
pca = PCA(n_components=2)
pca_result = pca.fit_transform(cosine_sim)
print(f"tfidf_matrix type: {type(tfidf_matrix)}")
print(f"tfidf_matrix shape: {tfidf_matrix.shape}")
tfidf_matrix type: <class 'scipy.sparse._csr.csr_matrix'>
tfidf_matrix shape: (360, 1558)
text_features = ['Ingredients']
print(f"text_features type: {type(text_features)}")
print(f"text_features shape: {text_features}")

[ ]
from google.colab import drive
drive.mount('/content/drive')
Mounted at /content/drive
[ ]
import os
os.environ["kaggle_com"]="/content/drive/MyDrive/Kaggle"
[ ]
%cd /content/drive/MyDrive/Kaggle
/content/drive/MyDrive/Kaggle
[ ]
import pandas as pd
df= pd.read_csv('/content/drive/MyDrive/Kaggle/Food (1).csv')
[ ]
df.head()

[ ]
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.decomposition import NMF
from sklearn.feature_extraction.text import TfidfVectorizer
[ ]
print(df.isnull().sum())  # Print the count of missing values in each column
print(df.duplicated().sum())  # Print the count of duplicate rows
Food_ID                111
Name                   111
C_Type                 111
Veg_Non                111
Calories               111
FatContent             111
CarbohydrateContent    111
FiberContent           111
SugarContent           111
ProteinContent         111
Diet Restriction       111
Seasonality            111
Ingredients            111
User_ID                111
Rating                 111
Restaurant_Name        111
Address                111
dtype: int64
110
[ ]
import matplotlib.pyplot as plt

labels = ['A', 'B', 'C', 'D', 'E']
values = [10, 15, 7, 12, 9]

plt.bar(labels, values)
plt.xlabel('Categories')
plt.ylabel('Values')
plt.title('Bar Chart')
plt.show()

[ ]
Ratings = df['Rating'].tolist()
[ ]
import pandas as pd
import matplotlib.pyplot as plt
[ ]
# Visualize the rating distribution
plt.hist(Ratings, bins=5, edgecolor='black')
plt.xlabel('Rating')
plt.ylabel('Frequency')
plt.title('Rating Distribution')
plt.show()

[ ]
Q1 = df['Calories'].quantile(0.25)
Q3 = df['Calories'].quantile(0.75)
IQR = Q3 - Q1
df = df[(df['Calories'] > Q1 - 1.5 * IQR) & (df['Calories'] < Q3 + 1.5 * IQR)]
[ ]
!pip install scikit-learn
!pip install pandas
Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)
Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)
Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)
Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)
Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.3.0)
Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)
Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)
Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)
Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)
Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)
[ ]
from sklearn.decomposition import PCA
[ ]
tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix = tfidf_vectorizer.fit_transform(df['Name'] + ' ' + df['C_Type'] + ' ' + df['Ingredients'] + ' ' + df['Diet Restriction'] + ' ' + df['Seasonality'])
[ ]
cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)
pca = PCA(n_components=2)
pca_result = pca.fit_transform(cosine_sim)
[ ]
print(f"tfidf_matrix type: {type(tfidf_matrix)}")
print(f"tfidf_matrix shape: {tfidf_matrix.shape}")
tfidf_matrix type: <class 'scipy.sparse._csr.csr_matrix'>
tfidf_matrix shape: (360, 1558)
[ ]
text_features = ['Ingredients']
[ ]
print(f"text_features type: {type(text_features)}")
print(f"text_features shape: {text_features}")
text_features type: <class 'list'>
text_features shape: ['Ingredients']
print(f"df type: {type(df)}")
print(f"df shape: {df.shape}")
df type: <class 'pandas.core.frame.DataFrame'>
df shape: (360, 17)
from sklearn.metrics.pairwise import linear_kernel
def content_based_filtering(input_data, tfidf_matrix, text_features, df):
    # Calculate TF-IDF vectors for the input data
    input_tfidf = tfidf_vectorizer.transform([' '.join(input_data)])

    # Calculate cosine similarities between input data and existing data
    cosine_similarities = linear_kernel(input_tfidf, tfidf_matrix).flatten()

    # Get indices of the most similar food items
    similar_food_indices = cosine_similarities.argsort()[:-6:-1]

    # Get the most similar food items from the dataframe
    similar_food = df.iloc[similar_food_indices]

    # Create a clear and neat output format for the recommendations
    recommendations = []
    for index, row in similar_food.iterrows():
        recommendation = {
            'Name': row['Name'],
            'C_Type': row['C_Type'],
            'Veg_Non': row['Veg_Non'],
            'Diet Restriction': row['Diet Restriction'],
            'Seasonality': row['Seasonality'],
            'Rating': row['Rating'],
            'Calories': row['Calories'],
            'FatContent': row['FatContent'],
            'CarbohydrateContent': row['CarbohydrateContent'],
            'FiberContent': row['FiberContent'],
            'SugarContent': row['SugarContent'],
            'ProteinContent': row['ProteinContent'],
            'Restaurant_Name': row['Restaurant_Name'],
            'Address': row['Address']
        }
        recommendations.append(recommendation)

    return recommendations
def content_based_filtering_with_allergens(input_data, tfidf_matrix, text_features, df):
    # Ask the user for their food allergens
    user_allergens = input("Please enter your food allergens (comma-separated): ").split(",")

    # Check for food allergens
    non_allergen_food = df[~df['Name'].isin(user_allergens)]

    # Calculate TF-IDF vectors for the input data
    input_tfidf = tfidf_vectorizer.transform([' '.join(input_data)])

    # Calculate cosine similarities between input data and existing data
    cosine_similarities = linear_kernel(input_tfidf, tfidf_matrix).flatten()

    # Get indices of the most similar non-allergen food items
    similar_food_indices = cosine_similarities.argsort()[:-6:-1]

    # Get the most similar non-allergen food items from the dataframe
    similar_food = non_allergen_food.iloc[similar_food_indices]

    # Create a clear and neat output format for the recommendations
    recommendations = []
    for index, row in similar_food.iterrows():
        recommendation = {
            'Name': row['Name'],
            'C_Type': row['C_Type'],
            'Veg_Non': row['Veg_Non'],
            'Diet Restriction': row['Diet Restriction'],
            'Seasonality': row['Seasonality'],
            'Rating': row['Rating'],
            'Calories': row['Calories'],
            'FatContent': row['FatContent'],
            'CarbohydrateContent': row['CarbohydrateContent'],
            'FiberContent': row['FiberContent'],
            'SugarContent': row['SugarContent'],
            'ProteinContent': row['ProteinContent'],
            'Restaurant_Name': row['Restaurant_Name'],
            'Total Ingredients':row ['Ingredients'],
            'Address': row['Address']
        }
        recommendations.append(recommendation)

    return recommendations
from sklearn.neighbors import NearestNeighbors

def collaborative_filtering(user_ratings, n_components=10):
    # Build the user-item matrix
    user_food_matrix = user_ratings.pivot(index='User_ID', columns='Food_ID', values='Rating').fillna(0)

    # Use NearestNeighbors or other collaborative filtering techniques to find similar users
    nn_model = NearestNeighbors(n_neighbors=5, algorithm='brute', metric='cosine')
    nn_model.fit(user_food_matrix)

    # Get the nearest neighbors for each user
    similar_users = nn_model.kneighbors(user_food_matrix, return_distance=False)

    # Get preferences for unseen food items based on similar users
    preferences = pd.DataFrame(0, index=user_food_matrix.index, columns=range(max(user_food_matrix.columns)+1))
    for user_index, similar_user_indices in enumerate(similar_users):
        for food_id in range(max(user_food_matrix.columns)+1):
            if food_id in user_food_matrix.columns and food_id not in preferences.columns:
                preferences[food_id] = user_food_matrix.iloc[:, user_food_matrix.columns == food_id].mean(axis=1)
            elif food_id in preferences.columns:
                preferences[food_id].iloc[user_index] = user_food_matrix.iloc[similar_user_indices, preferences.columns == food_id].mean()

    return preferences
def hybrid_filtering(content_based_recommendations, collaborative_filtering_recommendations, weights=(0.5, 0.5)):
    # Merge the content-based and collaborative filtering recommendations on 'Food_ID'
    final_recommendations = content_based_recommendations.merge(collaborative_filtering_recommendations, on='Food_ID', suffixes=('_content', '_collab'))

    # Calculate the final score using weighted averaging
    final_recommendations['final_score'] = final_recommendations['Rating_content'] * weights[0] + final_recommendations['Rating_collab'] * weights[1]

    return final_recommendations
recommendations = content_based_filtering_with_allergens(['Green Chilli', 'garlic paste', 'egg'], tfidf_matrix, text_features, df)
for i, recommendation in enumerate(recommendations, 1):
    print(f'Recommendation {i}:')
    for key, value in recommendation.items():
        print(f"{key}: {value}")
    print()
# Example usage: Get similar foods based on input ingredients 'Green Chilli', 'garlic paste', 'Chicken'
recommendations = content_based_filtering(['Green Chilli', 'garlic paste', 'Chicken'], tfidf_matrix, text_features, df)
for i, recommendation in enumerate(recommendations, 1):
    print(f'Recommendation {i}:')
    for key, value in recommendation.items():
        print(f"{key}: {value}")
    print()
# Example usage: Get similar foods based on input ingredients 'Green Chilli', 'garlic paste', 'Chicken'
recommendations = content_based_filtering( ['black olive','green capsicum','carrots'], tfidf_matrix, text_features, df)
for i, recommendation in enumerate(recommendations, 1):
    print(f'Recommendation {i}:')
    for key, value in recommendation.items():
        print(f"{key}: {value}")
    print()
from sklearn.metrics import mean_squared_error
Rating_x= [4, 5, 3, 2, 1]
Rating_y= [4.5, 4.8, 3.2, 2.7, 1.5]
rmse = np.sqrt(np.nanmean((np.array(Rating_x) - np.array(Rating_y)) ** 2))
mae = np.nanmean(np.abs(np.array(Rating_x) - np.array(Rating_y)))
# Assuming you have ground truth ratings for the recommended food items
def calculate_performance(recommendations, ground_truth_ratings):
    # Merge the recommendations with ground truth ratings on 'Food_ID'
    merged_data = recommendations.merge(ground_truth_ratings, on='Food_ID')

    # Calculate metrics such as RMSE, MAE, Precision, Recall, F1 Score, etc.
    # For example, calculating RMSE
    from sklearn.metrics import mean_squared_error
    rmse = mean_squared_error(merged_data['Rating'], merged_data['final_score'], squared=False)

    # You can calculate other metrics as needed based on your evaluation criteria

    return rmse
def calculate_performance(recommendations, ground_truth_ratings):
    # Merge the recommendations with ground truth ratings on 'Food_ID'
    merged_data = recommendations.merge(ground_truth_ratings, on='Food_ID')

    # Calculate metrics such as MSE, MAE, Precision, Recall, F1 Score, etc.
    # For example, calculating MSE
    mse = mean_squared_error(merged_data['Rating'], merged_data['final_score'])

    # You can calculate other metrics as needed based on your evaluation criteria

    return mse
print(f"RMSE: {rmse}")
print(f"MAE: {mae}")
import numpy as np
import matplotlib.pyplot as plt
labels = ['RMSE', 'MAE']
values = [rmse, mae]
plt.bar(labels, values, color=['blue', 'red'])
plt.xlabel('Error Metric')
plt.ylabel('Value')
plt.title('RMSE and MAE Comparison')
plt.show()

